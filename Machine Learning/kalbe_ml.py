# -*- coding: utf-8 -*-
"""kalbe_ml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_E4Q_VZQ0PYZKPRCF_w99gPNYK-a7Dl9
"""

# Import main package

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Data Ingestion

df_customer = pd.read_csv('Customer.csv', delimiter=';')
df_transaction = pd.read_csv('Transaction.csv', delimiter=';')
df_product = pd.read_csv('Product.csv', delimiter=';')
df_store = pd.read_csv('Store.csv', delimiter=';')

# Check the dim of the data

df_customer.shape, df_transaction.shape, df_product.shape, df_store.shape

"""# Preprocessing"""

df_customer.head()

df_customer.duplicated(subset=['CustomerID']).sum()

df_customer.isnull().sum()

df_customer[df_customer['Marital Status'].isnull()].index

df_customer.drop(df_customer[df_customer['Marital Status'].isnull()].index, inplace=True)

df_customer.shape

df_customer.dtypes

df_customer['Income'] = df_customer['Income'].replace('[,]', '.', regex=True).astype('float')

df_transaction.head()

df_transaction.duplicated(subset=['TransactionID']).sum()

df_transaction[df_transaction.duplicated(subset=['TransactionID'], keep='last')]

df_transaction[df_transaction['TransactionID'] == 'TR54287']

df_transaction[df_transaction.duplicated(subset=['TransactionID'], keep='last')].index

df_transaction.drop(df_transaction[df_transaction.duplicated(subset=['TransactionID'], keep='last')].index, inplace=True)

df_transaction.shape

df_transaction.isnull().sum()

df_transaction.dtypes

df_transaction['Date']

df_transaction['Date'] = pd.to_datetime(df_transaction['Date'], dayfirst=True)

df_transaction.dtypes

df_store.isnull().sum()

df_store.duplicated(subset=['StoreID']).sum()

df_store['Latitude'] = df_store['Latitude'].replace('[,]', '.', regex=True).astype('float')
df_store['Longitude'] = df_store['Longitude'].replace('[,]', '.', regex=True).astype('float')

df_product.isnull().sum()

df_product.duplicated(subset=['ProductID']).sum()

"""# Merge the Data"""

# Merge the data into one data

df_merge = pd.merge(df_transaction, df_customer, on=['CustomerID'])
df_merge = pd.merge(df_merge, df_product.drop('Price', axis=1), on=['ProductID'])
df_merge = pd.merge(df_merge, df_store, on=['StoreID'])

df_merge.head()

df_merge.info()

df_merge.describe()

"""#Time Series"""

df_regression = df_merge.copy().groupby(['Date']).agg({'Qty' : 'sum'}).reset_index()

df_regression

from statsmodels.tsa.seasonal import seasonal_decompose

decomposed = seasonal_decompose(df_regression.set_index(['Date']))

plt.figure(figsize=(8,8), dpi=200)

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')

plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonal')

plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residual')

plt.tight_layout()

setpoint = round(df_regression.shape[0] * 0.75)
df_train = df_regression[:setpoint]
df_val = df_regression[setpoint:].reset_index(drop=True)

df_train.head()

df_val.head()

plt.figure(figsize=(20,5))
sns.lineplot(data=df_train, x=df_train['Date'], y=df_train['Qty']);
sns.lineplot(data=df_val, x=df_val['Date'], y=df_val['Qty']);

"""## Create an ARIMA model"""

from statsmodels.tsa.arima.model import ARIMA

df_train = df_train.set_index('Date')
df_val = df_val.set_index('Date')

y_train = df_train['Qty']
y_val = df_val['Qty']

arima_model = ARIMA(y_train, order = (40,1,0))
arima_model = arima_model.fit( )

y_pred = arima_model.get_forecast(len(df_val))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = arima_model.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df.index = df_val.index
y_pred_final = y_pred_df['predictions']

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_val, y_pred_final)
rmse = np.sqrt(mean_squared_error(y_val, y_pred_final))

plt.figure(figsize=(20,5), dpi=200)
plt.plot(y_train)
plt.plot(y_val, color='orange')
plt.plot(y_pred_final, color='black', label='ARIMA Predictions')
plt.legend();

"""# Clustering

### Data Ingestion
"""

df_clustering = df_merge.copy().groupby(['CustomerID']).agg({'TransactionID' : 'count', 'Qty' : 'sum', 'TotalAmount' : 'sum'}).reset_index()

df_clustering.head()

df_clustering.info()

df_clustering.describe()

"""## Exploratory Data Analysis"""

sns.scatterplot(data=df_clustering, x='Qty', y='TotalAmount')

sns.histplot(data=df_clustering, x='Qty')

sns.histplot(data=df_clustering, x='TotalAmount')

"""## Feature Selection

Drop the Unique Feature, because it wont be used
"""

X = df_clustering.copy().drop(['TransactionID', 'CustomerID'], axis=1)

X.head()

"""## Scaling the Data"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_X = scaler.fit_transform(X)

scaled_X

"""## Modelling and Predictions"""

from sklearn.cluster import KMeans

ssd = []

for k in range(2,30):

    model = KMeans(n_clusters=k)


    model.fit(scaled_X)

    #Sum of squared distances of samples to their closest cluster center.
    ssd.append(model.inertia_)

plt.plot(range(2,30),ssd,'o--')
plt.xlabel("K Value")
plt.ylabel(" Sum of Squared Distances")

pd.Series(ssd).diff().plot(kind='bar')

model = KMeans(n_clusters=3)
model.fit(scaled_X)

model.labels_

df_clustering['Clusters'] = model.labels_

df_clustering

df_clustering.groupby(['Clusters']).agg({'CustomerID': 'count', 'Qty': 'mean', 'TotalAmount': 'mean'})

